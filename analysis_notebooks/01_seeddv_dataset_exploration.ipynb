{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# SEED-DV Dataset EEG Data Exploration\n\nThis notebook is dedicated to exploring EEG data in the SEED-DV dataset, including:\n- Loading and analyzing EEG data from 20 subjects\n- Basic shape, channel count, and time dimensions of EEG signals\n- Data quality checks and statistical analysis\n- EEG channel distribution and signal characteristics\n- Comparative analysis between subjects"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nimport os\nimport sys\nfrom glob import glob\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Add project root directory to Python path\nproject_root = Path('.').absolute().parent\nsys.path.append(str(project_root))\n\n# Set matplotlib display parameters\nplt.rcParams['figure.figsize'] = [12, 8]\nplt.rcParams['font.size'] = 10\nsns.set_style(\"whitegrid\")\n\n# Directly specify SEED-DV dataset path\n# Please modify this path according to your actual situation\nSEED_DV_DATA_PATH = project_root / 'data' / 'SEED-DV'/'EEG'\n\nprint(f\"Project root directory: {project_root}\")\nprint(f\"Current working directory: {os.getcwd()}\")\nprint(f\"SEED-DV data path: {SEED_DV_DATA_PATH}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. SEED-DV Dataset Path Location"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check if SEED-DV data path exists\nprint(\"=== SEED-DV Dataset Check ===\")\n\nif not SEED_DV_DATA_PATH.exists():\n    print(f\"❌ Data path does not exist: {SEED_DV_DATA_PATH}\")\n    print(\"Please confirm:\")\n    print(\"1. SEED-DV dataset has been downloaded\")\n    print(\"2. Dataset is placed in the correct location\")\n    print(\"3. Modify SEED_DV_DATA_PATH in the cell above to the correct path\")\nelse:\n    print(f\"✅ Found data path: {SEED_DV_DATA_PATH}\")\n    \n    # Search for all .npy files\n    npy_files = list(SEED_DV_DATA_PATH.rglob('*.npy'))\n    \n    print(f\"\\nFound .npy files: {len(npy_files)} files\")\n    \n    if len(npy_files) > 0:\n        print(\"First 10 files:\")\n        for i, npy_file in enumerate(npy_files[:10]):\n            size_mb = npy_file.stat().st_size / (1024 * 1024)\n            print(f\"  {i+1:2d}. {npy_file.name} ({size_mb:.2f} MB)\")\n        \n        if len(npy_files) > 10:\n            print(f\"  ... and {len(npy_files)-10} more files\")\n            \n        # Save file list for subsequent use\n        subject_files = sorted(npy_files)\n        print(f\"\\n✅ Ready to analyze {len(subject_files)} data files\")\n    else:\n        print(\"❌ No .npy data files found\")\n        subject_files = []"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Load and Analyze Data from the First Subject"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the first file as a sample for analysis\nif subject_files:\n    sample_file = subject_files[0]\n    print(f\"=== Analyzing sample file: {sample_file.name} ===\")\n    \n    try:\n        # Load data\n        sample_data = np.load(sample_file)\n        \n        print(f\"Data type: {type(sample_data)}\")\n        print(f\"Data shape: {sample_data.shape}\")\n        print(f\"Data dtype: {sample_data.dtype}\")\n        print(f\"Data size: {sample_data.nbytes / (1024*1024):.2f} MB\")\n        \n        if sample_data.ndim >= 2:\n            print(f\"\\nData dimension analysis:\")\n            for i, dim_size in enumerate(sample_data.shape):\n                print(f\"  Dimension {i}: {dim_size}\")\n                \n        print(f\"\\nData statistics:\")\n        print(f\"  Min value: {sample_data.min():.6f}\")\n        print(f\"  Max value: {sample_data.max():.6f}\")\n        print(f\"  Mean value: {sample_data.mean():.6f}\")\n        print(f\"  Standard deviation: {sample_data.std():.6f}\")\n        \n        # Check for NaN or inf values\n        nan_count = np.isnan(sample_data).sum()\n        inf_count = np.isinf(sample_data).sum()\n        print(f\"\\nData quality check:\")\n        print(f\"  Number of NaN values: {nan_count}\")\n        print(f\"  Number of Inf values: {inf_count}\")\n        \n        # Save sample data for subsequent analysis\n        globals()['sample_eeg_data'] = sample_data\n        \n    except Exception as e:\n        print(f\"Failed to load data: {e}\")\n        sample_data = None\nelse:\n    print(\"No data files found for analysis\")\n    sample_data = None"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. EEG Data Structure Inference"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if 'sample_eeg_data' in globals() and sample_eeg_data is not None:\n    print(\"=== EEG Data Structure Analysis ===\")\n    \n    data_shape = sample_eeg_data.shape\n    \n    # Infer structure based on data shape\n    if len(data_shape) == 2:\n        print(\"Data format: 2D array\")\n        print(\"Possible structures:\")\n        print(f\"  - (time points, channels): {data_shape[0]} × {data_shape[1]}\")\n        print(f\"  - (channels, time points): {data_shape[0]} × {data_shape[1]}\")\n        \n        # Usually EEG data has 8-128 channels\n        if 8 <= data_shape[0] <= 128 and data_shape[1] > data_shape[0]:\n            channels, time_points = data_shape[0], data_shape[1]\n            print(f\"\\nInferred structure: (channels={channels}, time points={time_points})\")\n        elif 8 <= data_shape[1] <= 128 and data_shape[0] > data_shape[1]:\n            time_points, channels = data_shape[0], data_shape[1]\n            print(f\"\\nInferred structure: (time points={time_points}, channels={channels})\")\n        else:\n            print(f\"\\nUnable to determine structure, further analysis needed\")\n            \n    elif len(data_shape) == 3:\n        print(\"Data format: 3D array\")\n        print(\"Possible structures:\")\n        print(f\"  - (trials, channels, time): {data_shape[0]} × {data_shape[1]} × {data_shape[2]}\")\n        print(f\"  - (trials, time, channels): {data_shape[0]} × {data_shape[1]} × {data_shape[2]}\")\n        \n        # Infer most likely structure\n        if 8 <= data_shape[1] <= 128:\n            trials, channels, time_points = data_shape\n            print(f\"\\nInferred structure: (trials={trials}, channels={channels}, time points={time_points})\")\n        elif 8 <= data_shape[2] <= 128:\n            trials, time_points, channels = data_shape\n            print(f\"\\nInferred structure: (trials={trials}, time points={time_points}, channels={channels})\")\n            \n    elif len(data_shape) == 4:\n        print(\"Data format: 4D array\")\n        print(f\"Possible structure: (subjects/sessions, trials, channels, time) or similar combinations\")\n        print(f\"Dimension sizes: {data_shape}\")\n        \n    # If this is SEED-DV, estimate sampling rate\n    print(f\"\\n=== Sampling Rate Inference ===\")\n    print(\"SEED-DV dataset typically uses 200Hz sampling rate\")\n    if len(data_shape) >= 2:\n        max_dim = max(data_shape)\n        min_dim = min(data_shape)\n        \n        # Assume the largest dimension is time points\n        if max_dim > 1000:  # Likely time points\n            estimated_duration = max_dim / 200  # Assume 200Hz\n            print(f\"Assuming 200Hz sampling rate, data duration approximately: {estimated_duration:.1f} seconds\")\n            \n        # Assume the smallest dimension might be channel count\n        if 8 <= min_dim <= 128:\n            print(f\"Estimated number of channels: {min_dim}\")\nelse:\n    print(\"No sample data available for analysis\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Visualize EEG Data from the First Subject"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if 'sample_eeg_data' in globals() and sample_eeg_data is not None:\n    print(\"=== EEG Data Visualization ===\")\n    \n    # Create multiple subplots\n    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n    \n    # 1. Data shape overview\n    ax1 = axes[0, 0]\n    if sample_eeg_data.ndim == 2:\n        # Display heatmap of entire data (downsampled version)\n        if sample_eeg_data.shape[0] < sample_eeg_data.shape[1]:\n            # Assume (channels, time)\n            downsampled = sample_eeg_data[:, ::max(1, sample_eeg_data.shape[1]//1000)]\n            im1 = ax1.imshow(downsampled, aspect='auto', cmap='RdBu_r')\n            ax1.set_title('EEG Data Overview (Channels × Time)')\n            ax1.set_xlabel('Time (samples)')\n            ax1.set_ylabel('Channels')\n        else:\n            # Assume (time, channels)\n            downsampled = sample_eeg_data[::max(1, sample_eeg_data.shape[0]//1000), :]\n            im1 = ax1.imshow(downsampled.T, aspect='auto', cmap='RdBu_r')\n            ax1.set_title('EEG Data Overview (Channels × Time)')\n            ax1.set_xlabel('Time (samples)')\n            ax1.set_ylabel('Channels')\n        plt.colorbar(im1, ax=ax1, shrink=0.6)\n    \n    # 2. Single channel time series\n    ax2 = axes[0, 1]\n    if sample_eeg_data.ndim >= 2:\n        # Select first channel for display\n        if sample_eeg_data.shape[0] < sample_eeg_data.shape[1]:\n            # (channels, time)\n            channel_data = sample_eeg_data[0, :min(2000, sample_eeg_data.shape[1])]  # Display first 2000 points\n            time_axis = np.arange(len(channel_data)) / 200  # Assume 200Hz\n        else:\n            # (time, channels)\n            channel_data = sample_eeg_data[:min(2000, sample_eeg_data.shape[0]), 0]\n            time_axis = np.arange(len(channel_data)) / 200\n        \n        ax2.plot(time_axis, channel_data, 'b-', linewidth=0.8)\n        ax2.set_title('Channel 1 EEG Signal')\n        ax2.set_xlabel('Time (seconds)')\n        ax2.set_ylabel('Amplitude')\n        ax2.grid(True, alpha=0.3)\n    \n    # 3. Data distribution histogram\n    ax3 = axes[1, 0]\n    # Sample part of data for histogram\n    sample_for_hist = sample_eeg_data.flatten()\n    if len(sample_for_hist) > 10000:\n        sample_for_hist = np.random.choice(sample_for_hist, 10000, replace=False)\n    \n    ax3.hist(sample_for_hist, bins=50, alpha=0.7, edgecolor='black')\n    ax3.set_title('EEG Data Distribution')\n    ax3.set_xlabel('Amplitude')\n    ax3.set_ylabel('Frequency')\n    ax3.grid(True, alpha=0.3)\n    \n    # 4. Inter-channel correlation (if data is not too large)\n    ax4 = axes[1, 1]\n    if sample_eeg_data.ndim == 2:\n        if sample_eeg_data.shape[0] <= 64 and sample_eeg_data.shape[1] > sample_eeg_data.shape[0]:\n            # (channels, time) - calculate inter-channel correlation\n            corr_matrix = np.corrcoef(sample_eeg_data)\n            im4 = ax4.imshow(corr_matrix, cmap='coolwarm', vmin=-1, vmax=1)\n            ax4.set_title('Inter-channel Correlation')\n            ax4.set_xlabel('Channels')\n            ax4.set_ylabel('Channels')\n            plt.colorbar(im4, ax=ax4, shrink=0.6)\n        else:\n            # Display power spectral density estimate\n            if sample_eeg_data.shape[0] > sample_eeg_data.shape[1]:\n                # (time, channels)\n                signal = sample_eeg_data[:min(1000, sample_eeg_data.shape[0]), 0]\n            else:\n                # (channels, time)\n                signal = sample_eeg_data[0, :min(1000, sample_eeg_data.shape[1])]\n            \n            from scipy.signal import welch\n            freqs, psd = welch(signal, fs=200, nperseg=min(256, len(signal)//4))\n            ax4.semilogy(freqs, psd)\n            ax4.set_title('Power Spectral Density (Channel 1)')\n            ax4.set_xlabel('Frequency (Hz)')\n            ax4.set_ylabel('Power Spectral Density')\n            ax4.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \nelse:\n    print(\"No data available for visualization\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Batch Analysis of Multiple Subject Data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if subject_files:\n    print(\"=== Batch Analysis of Subject Data ===\")\n    \n    # Analyze first few files (avoid memory overflow)\n    max_files_to_analyze = min(10, len(subject_files))\n    \n    analysis_results = []\n    \n    for i, file_path in enumerate(subject_files[:max_files_to_analyze]):\n        try:\n            print(f\"Analyzing file {i+1}/{max_files_to_analyze}: {file_path.name}\", end=\" \")\n            \n            # Load data\n            data = np.load(file_path)\n            \n            # Collect basic information\n            result = {\n                'subject_id': i+1,\n                'filename': file_path.name,\n                'shape': data.shape,\n                'dtype': str(data.dtype),\n                'size_mb': data.nbytes / (1024*1024),\n                'min_val': data.min(),\n                'max_val': data.max(),\n                'mean_val': data.mean(),\n                'std_val': data.std(),\n                'nan_count': np.isnan(data).sum(),\n                'inf_count': np.isinf(data).sum()\n            }\n            \n            analysis_results.append(result)\n            print(\"✅\")\n            \n        except Exception as e:\n            print(f\"❌ Error: {e}\")\n            continue\n    \n    # Create results DataFrame\n    if analysis_results:\n        results_df = pd.DataFrame(analysis_results)\n        \n        print(f\"\\n=== Analysis Results Summary ({len(analysis_results)} files) ===\")\n        print(\"\\nBasic file information:\")\n        print(results_df[['subject_id', 'filename', 'shape', 'size_mb']].to_string(index=False))\n        \n        print(\"\\nData statistics:\")\n        stats_cols = ['min_val', 'max_val', 'mean_val', 'std_val']\n        print(results_df[['subject_id'] + stats_cols].round(4).to_string(index=False))\n        \n        # Check data consistency\n        print(f\"\\n=== Data Consistency Check ===\")\n        unique_shapes = results_df['shape'].unique()\n        print(f\"Different data shapes: {len(unique_shapes)} types\")\n        for shape in unique_shapes:\n            count = (results_df['shape'] == shape).sum()\n            print(f\"  {shape}: {count} files\")\n        \n        unique_dtypes = results_df['dtype'].unique()\n        print(f\"\\nDifferent data types: {len(unique_dtypes)} types\")\n        for dtype in unique_dtypes:\n            count = (results_df['dtype'] == dtype).sum()\n            print(f\"  {dtype}: {count} files\")\n        \n        # Data quality check\n        nan_files = (results_df['nan_count'] > 0).sum()\n        inf_files = (results_df['inf_count'] > 0).sum()\n        print(f\"\\nData quality:\")\n        print(f\"  Files containing NaN values: {nan_files}\")\n        print(f\"  Files containing Inf values: {inf_files}\")\n        \n        # Save results for subsequent analysis\n        globals()['batch_analysis_results'] = results_df\n        \n    else:\n        print(\"No files were successfully analyzed\")\n        \nelse:\n    print(\"No subject data files found\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Data Shape and Structure Comparison Visualization"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if 'batch_analysis_results' in globals():\n    print(\"=== Subject Data Comparison Visualization ===\")\n    \n    df = batch_analysis_results\n    \n    # Create comparison plots\n    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n    \n    # 1. File size comparison\n    ax1 = axes[0, 0]\n    ax1.bar(df['subject_id'], df['size_mb'])\n    ax1.set_title('Data File Size by Subject')\n    ax1.set_xlabel('Subject ID')\n    ax1.set_ylabel('File Size (MB)')\n    ax1.grid(True, alpha=0.3)\n    \n    # 2. Data mean comparison\n    ax2 = axes[0, 1]\n    ax2.bar(df['subject_id'], df['mean_val'])\n    ax2.set_title('Data Mean by Subject')\n    ax2.set_xlabel('Subject ID')\n    ax2.set_ylabel('Data Mean')\n    ax2.grid(True, alpha=0.3)\n    \n    # 3. Data standard deviation comparison\n    ax3 = axes[0, 2]\n    ax3.bar(df['subject_id'], df['std_val'])\n    ax3.set_title('Data Standard Deviation by Subject')\n    ax3.set_xlabel('Subject ID')\n    ax3.set_ylabel('Standard Deviation')\n    ax3.grid(True, alpha=0.3)\n    \n    # 4. Data range comparison\n    ax4 = axes[1, 0]\n    width = 0.35\n    x = df['subject_id']\n    ax4.bar(x - width/2, df['min_val'], width, label='Min Value', alpha=0.7)\n    ax4.bar(x + width/2, df['max_val'], width, label='Max Value', alpha=0.7)\n    ax4.set_title('Data Range by Subject')\n    ax4.set_xlabel('Subject ID')\n    ax4.set_ylabel('Value')\n    ax4.legend()\n    ax4.grid(True, alpha=0.3)\n    \n    # 5. Data distribution overview\n    ax5 = axes[1, 1]\n    ax5.boxplot([df['mean_val'], df['std_val'], df['min_val'], df['max_val']], \n                labels=['Mean', 'Std Dev', 'Min Value', 'Max Value'])\n    ax5.set_title('Distribution of Statistical Measures')\n    ax5.set_ylabel('Value')\n    ax5.grid(True, alpha=0.3)\n    \n    # 6. Correlation heatmap\n    ax6 = axes[1, 2]\n    numeric_cols = ['size_mb', 'mean_val', 'std_val', 'min_val', 'max_val']\n    corr_matrix = df[numeric_cols].corr()\n    im = ax6.imshow(corr_matrix, cmap='coolwarm', vmin=-1, vmax=1)\n    ax6.set_xticks(range(len(numeric_cols)))\n    ax6.set_yticks(range(len(numeric_cols)))\n    ax6.set_xticklabels([col.replace('_', '\\n') for col in numeric_cols], rotation=45)\n    ax6.set_yticklabels([col.replace('_', '\\n') for col in numeric_cols])\n    ax6.set_title('Correlation Between Statistics')\n    \n    # Add correlation coefficient text\n    for i in range(len(numeric_cols)):\n        for j in range(len(numeric_cols)):\n            text = ax6.text(j, i, f'{corr_matrix.iloc[i, j]:.2f}',\n                           ha=\"center\", va=\"center\", color=\"black\" if abs(corr_matrix.iloc[i, j]) < 0.5 else \"white\")\n    \n    plt.tight_layout()\n    plt.show()\n    \nelse:\n    print(\"No batch analysis results available for visualization\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. SEED-DV Dataset Summary and Recommendations"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=== SEED-DV EEG Dataset Analysis Summary ===\")\n\nif 'batch_analysis_results' in globals() and len(batch_analysis_results) > 0:\n    df = batch_analysis_results\n    \n    print(f\"\\n📊 Basic Dataset Information:\")\n    print(f\"  • Number of subjects: {len(df)} (analyzed first {len(df)} files)\")\n    print(f\"  • Total data size: {df['size_mb'].sum():.2f} MB\")\n    print(f\"  • Average file size: {df['size_mb'].mean():.2f} ± {df['size_mb'].std():.2f} MB\")\n    \n    # Data shape analysis\n    most_common_shape = df['shape'].mode().iloc[0] if len(df) > 0 else None\n    if most_common_shape:\n        print(f\"\\n📐 Data Structure:\")\n        print(f\"  • Most common data shape: {most_common_shape}\")\n        \n        # Infer based on known SEED-DV information\n        if len(most_common_shape) == 3:\n            print(f\"  • Inferred as 3D format: (trials/epochs, channels, timepoints)\")\n            print(f\"    - Number of trials: {most_common_shape[0]}\")\n            print(f\"    - Number of channels: {most_common_shape[1]}\")\n            print(f\"    - Number of timepoints: {most_common_shape[2]}\")\n            \n            if most_common_shape[2] > 1000:  # Likely timepoints\n                estimated_duration = most_common_shape[2] / 200  # SEED-DV typically 200Hz\n                print(f\"    - Estimated sampling rate: 200 Hz\")\n                print(f\"    - Estimated duration per trial: {estimated_duration:.1f} seconds\")\n        \n        elif len(most_common_shape) == 2:\n            print(f\"  • Inferred as 2D format: (timepoints, channels) or (channels, timepoints)\")\n    \n    print(f\"\\n📈 Data Quality:\")\n    clean_files = len(df[(df['nan_count'] == 0) & (df['inf_count'] == 0)])\n    print(f\"  • Completely clean files: {clean_files}/{len(df)} ({clean_files/len(df)*100:.1f}%)\")\n    \n    if df['nan_count'].sum() > 0:\n        print(f\"  ⚠️  Found NaN values, preprocessing needed\")\n    if df['inf_count'].sum() > 0:\n        print(f\"  ⚠️  Found Inf values, preprocessing needed\")\n    \n    print(f\"\\n📊 Data Statistics:\")\n    print(f\"  • Value range: [{df['min_val'].min():.3f}, {df['max_val'].max():.3f}]\")\n    print(f\"  • Average amplitude: {df['mean_val'].mean():.3f} ± {df['mean_val'].std():.3f}\")\n    print(f\"  • Average standard deviation: {df['std_val'].mean():.3f} ± {df['std_val'].std():.3f}\")\n\nelse:\n    print(\"\\n❌ Failed to successfully analyze data files\")\n\nprint(f\"\\n🔍 Recommendations for Further Analysis:\")\nprint(f\"  1. Confirm exact data format (trial × channel × time or other)\")\nprint(f\"  2. Verify sampling rate (SEED-DV typically 200Hz)\")\nprint(f\"  3. Confirm EEG channel layout and standard (e.g., 10-20 system)\")\nprint(f\"  4. Check preprocessing status of data (filtering, denoising, etc.)\")\nprint(f\"  5. Analyze stimulus labels and corresponding EEG responses\")\nprint(f\"  6. Perform frequency domain analysis (α, β, γ, θ, δ bands)\")\nprint(f\"  7. Examine individual differences between subjects\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}